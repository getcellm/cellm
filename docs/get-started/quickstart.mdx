---
title: 'Quickstart'
description: 'Get up and running with Cellm in minutes'
icon: 'rocket'
---

## Install 
To get started, you can quickly install Cellm and a local model:

1. Go to the [Github Release page](https://github.com/getcellm/cellm/releases) and download `Cellm-AddIn-Release-x64.msi`
2. Run the installer
3. Download and install [Ollama](https://ollama.com/)
4. Open the Windows Terminal and type `ollama pull gemma3:4b-it-qat`

<Info>
  Cellm requires Windows 10 or higher, [Excel 2010 or higher](https://www.microsoft.com/en-us/microsoft-365/excel) (must be desktop app), and the [.NET 9.0 Runtime](https://dotnet.microsoft.com/en-us/download/dotnet/9.0). See the [Install](install.mdx) page for details.
</Info>

## Cellm
After installation, open Excel and look for the new **Cellm** tab in the ribbon menu. Use this tab to configure Cellm:

<Frame caption="Cellm Ribbon menu">
  <img src="https://assets.getcellm.com/docs/images/get-started/cellm-ribbon.png" />
</Frame>

This menu gives you control over how Cellm works. You can change which AI model to use, adjust settings like creativity level, and enable tools that give the AI access to your files or the web.

<AccordionGroup>
  <Accordion icon="file-excel" title="Account">
    **Login or logout of your Cellm account**
    
    When logged in, you can use the Cellm model provider to have Cellm pay for your model usage via your Cellm subscription.
  </Accordion>
  <Accordion icon="file-excel" title="Model">
    **Select which AI model to use**
    
    - **Provider**: Use the provider dropdown to change the default model provider (the one used when you run `=PROMPT()` formulas). You can click on the icon to change the API URL and API key.
    - **Model**: Use the model dropdown to change the default model either by selecting from a couple of presets or by typing a model name directly into the text box.
    - **Temperature**: Use the temperature dropdown to balance model consistency and creativity. The temperature is a number between 0.0 and 1.0 where 0.0 will almost always give you the same result if the same prompt is run multiple times. A temperature of 1.0 will give you very varied responses. "Consistent" corresponds to 0.0, "Neutral" corresponds to 0.3, and "Creative" corresponds to 0.7. You also write numbers directly in the text box if you need other values.
    - **Usage**: Shows the number of input and output tokens spent this session as well as how many prompts you have sent.
    - **Speed**: Shows the average Tokens Per Second (TPS) per request and the average Requests Per Second (TPS) during the last 30 seconds. Periods without any active requests during the last 30 seconds are not included in the calculation.
    - **Memory**: Toggles caching of model responses on and off. If memory is on, model responses are cached such the previous response is returned right away if prompt is run a second time. This helps you avoid paying for duplicate requests. 
  </Accordion>
  <Accordion icon="file-excel" title="Output Shape">
    **Insert the `=PROMPT()` formula in the active cell and open the function wizard**
    
    - **Cell**: Inserts the `=PROMPT()` formula which outputs the model's response in a single cell.
    - **Row**: Inserts the `=PROMPT.TOROW()` formula which spills multiple output values (if any) into cells to the right.
    - **Column**: Inserts the `=PROMPT.TOCOLUMN()` formula which spills multiple output values (if any) into cells below.
    - **Range**: Inserts the `=PROMPT.TORANGE()` formula which lets the model decide whether to spill multiple values (if any) into rows and/or columns or not.

    If the active cell already contains a `=PROMPT()`, the output shape changes while arguments remains the same.
  </Accordion>
  <Accordion icon="file-excel" title="Tools">
    Enable or disable built-in functions and add, edit, or remove Model Context Protocol (MCP) servers.
  </Accordion>
</AccordionGroup>

## Usage

### Sending a prompt to a model
You can use cell references to augment your prompt with data in your spreadsheet. Select a cell and type e.g. `=PROMPT("Extract the main topic from: The quarterly sales report shows a 15% increase in revenue across all
  departments")`. The model will tell you that the main topic is sales. By default, Cellm is configured to use Ollama with the Gemma 3 4B model, which is a small model that runs on your CPU.

### Using context in other cells
For example, try to copy a news article into cell A1 and type in the formula of cell B1 `=PROMPT("Extract all person names mentioned in the text")`. Then copy another news article into cell A1 and watch the formula recalcute itself and output the new persons names.

You can also reference many cells using standard Excel notation, e.g. `=PROMPT(A1:F10, "Extract all person names in the cells")`.

You can use cell references to dynamically change your prompts based on other data in your spreadsheet. Since `=PROMPT()` behaves like a normal Excel formula, you can drag the cell to apply the prompt separately to many rows. Try copy a list of news articles into column A and type in cell B1 `=PROMPT(A1, "Extract all person names mentioned in the text")`. Then drag the fill handle down to apply the formula to all rows. Cellm will automatically send each article to the model and output the results or corresponding cells in column B.

### Spilling model response across multiple cells
You can use the `=PROMPT.TOCOLUMN()` function to spill the model response across multiple cells in column B. For example, if you have a news article in cell A1, you can type in cell B1: `=PROMPT.TOCOLUMN(A1, "Generate 10 relevant hashtags for an instagram post about remote work")` to put multiple names across separate cells below. 

<Frame caption="Format output as a row or a column with `=PROMPT.TOROW()` and `=PROMPT.TOCOLUMN()`">
  <img src="https://assets.getcellm.com/docs/images/usage/writing-prompts/to-row-column.png" />
</Frame>

You can also use `=PROMPT.TOROW()` to spill the response across cells to the right or `=PROMPT.TORANGE()` to let the model choose whether to spill multiple values (if any) across rows and/or columns or not.

<Warning>
  If you wish to combine multi-value output with function calling, you must use OpenAI as no other provider supports combinging structured output with function calling. 
</Warning>

### Function calling
Beyond basic text processing, you can use "Function Calling" to give models access to external tools and data and even take action in other systems. For example, expand the "Functions" menu in the Cellm ribbon tab, and click on "File Search" to enable the model to search for files on your computer. Then type `=PROMPT(A1, "Which pdf files do I have in C:\Users\username\Downloads?")`. 

<Tip>
  Gemma 3 4B does not support function calling. For function calling you must use another model, e.g. OpenAI's `gpt-4o-mini`.
</Tip>

## Next steps

<CardGroup cols={2}>
  <Card
    title="Prompting"
    icon="pen-to-square"
    href="/usage/prompting">
    Learn more about prompting techniques and building automated workflows. 
  </Card>
  <Card
    title="Breaking Down Tasks for AI"
    icon="lightbulb"
    href="/usage/breaking-down-tasks">
    Learn how to split a complex task into multiple smaller ones and select the right model for your tasks' complexity.
  </Card>
  <Card
    title="Function Calling"
    icon="image"
    href="/usage/function-calling">
    Use Excel as a low-code task orchestrator by enabling Cellm to access external tools and data.
  </Card>
</CardGroup>
