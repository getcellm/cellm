---
title: 'Quickstart'
description: 'Get up and running with Cellm in minutes'
icon: 'rocket'
---

## Install
To get started, you can quickly install Cellm and a local model:

<Steps>
  <Step title="Download Cellm">
    Go to the [Github Release page](https://github.com/getcellm/cellm/releases) and download the latest `Cellm-AddIn-Release-x64.msi`
  </Step>
  <Step title="Install Cellm">
    Run the installer to install Cellm on your computer.
  </Step>
  <Step title="Install Ollama">
    Download and install [Ollama](https://ollama.com/) to run local AI models.
  </Step>
  <Step title="Download Gemma 3 model">
    Open the Windows Terminal and type `ollama pull gemma3:4b-it-qat` to download the Gemma 3 4B model.
  </Step>
</Steps>

<Info>
  Cellm requires Windows 10 or higher, [Excel 2010 or higher](https://www.microsoft.com/en-us/microsoft-365/excel) (must be desktop app), and the [.NET 9.0 Runtime](https://dotnet.microsoft.com/en-us/download/dotnet/9.0). See the [Install](install.mdx) page for details.
</Info>

## Cellm Ribbon
After installation, open Excel and look for the new **Cellm** tab in the ribbon menu. Use this tab to configure Cellm:

<Frame caption="Cellm Ribbon menu">
  <img src="https://assets.getcellm.com/docs/images/get-started/cellm-ribbon.png" />
</Frame>

The ribbon has four main sections that control how Cellm works:

<AccordionGroup>
  <Accordion icon="user" title="Account">
    **Manage your Cellm account**

    - **Login/Logout**: Log in to your Cellm account to use the Cellm provider, which handles model costs through your subscription
    - **Create Account**: Sign up for a Cellm account to access all features
    - **Manage Account**: Access your account settings
  </Accordion>

  <Accordion icon="message" title="Prompt">
    **Insert PROMPT formulas with different output shapes**

    Click these buttons to insert a `=PROMPT()` formula in the active cell and open the function wizard:

    - **Cell**: Output to a single cell (default)
    - **Row**: Spill multiple values across cells to the right with `=PROMPT.TOROW()`
    - **Column**: Spill multiple values down with `=PROMPT.TOCOLUMN()`
    - **Range**: Let the model decide the output shape with `=PROMPT.TORANGE()`

    If the active cell already contains a PROMPT formula, clicking these buttons changes the output shape while keeping your arguments.
  </Accordion>

  <Accordion icon="brain" title="Model">
    **Configure which AI model to use and how**

    - **Provider**: Select your model provider (OpenAI, Anthropic, Ollama, etc.). Click the provider icon to configure API keys and base URLs
    - **Model**: Choose from preset models or type a custom model name
    - **Temperature**: Control randomness (Consistent/0.0, Neutral/0.3, Creative/0.7, or any value 0.0-1.0)
    - **Usage**: View input/output tokens and prompt counts for this session
    - **Speed**: See average Tokens Per Second (TPS) and Requests Per Second (RPS)
    - **Memory**: Toggle response caching. When on, identical prompts return cached results instantly, saving costs
  </Accordion>

  <Accordion icon="wrench" title="Tools">
    **Enable tools that extend model capabilities**

    - **Functions**: Built-in tools the model can use:
      - **Internet Browser**: Let models browse the web
      - **File Search**: Let models search for files on your computer
      - **File Reader**: Let models read PDF, Markdown, and text files

    - **MCP**: Add, configure, or remove Model Context Protocol servers for advanced integrations
  </Accordion>
</AccordionGroup>

## Usage

### Sending a prompt to a model
Sending prompts to a model is easy. Select a cell and type:

````mdx Text instruction
=PROMPT("Extract the main topic from: The quarterly sales report shows a 15% increase in revenue across all departments")
````

The model will tell you that the main topic is sales. By default, Cellm is configured to use Ollama with the Gemma 3 4B model, which is a small model that runs on your CPU.

### Combine your prompt with data from your spreadsheet
You can process entire datasets by referencing cells in your prompts, letting the AI work with your actual spreadsheet data. For example, try to copy a news article into cell A1 and type in the formula of cell B1:

````mdx Single cell context
=PROMPT("Extract all person names mentioned in the text", A1)
````

Then copy another news article into cell A1 and watch the formula recalculate itself and output the new person names. You can also use cell references to dynamically change your prompts based on other data in your spreadsheet. You can drag the cell to apply the prompt separately to many rows. Try copy a list of news articles into column A and type in cell B1:

````mdx Dynamic prompts
=PROMPT("Extract all person names mentioned in the text", A1)
````

Then drag the fill handle down to apply the formula to all rows. Cellm will automatically send each article to the model and output the results in corresponding cells in column B.

You can also reference many cells using standard Excel notation:

````mdx Range context
=PROMPT("Extract all person names in the cells", A1:F10)
````

### Spilling model response across multiple cells
You can use the `=PROMPT.TOCOLUMN()` function to spill the model response across multiple cells. For example, if you have text in cell A1, you can type in cell B1:

````mdx Spill to column
=PROMPT.TOCOLUMN("Generate 10 relevant hashtags", A1)
````

This will put multiple values across separate cells below. 

<Frame caption="Format output as a row or a column with `=PROMPT.TOROW()` and `=PROMPT.TOCOLUMN()`">
  <img src="https://assets.getcellm.com/docs/images/usage/writing-prompts/to-row-column.png" />
</Frame>

You can also use `=PROMPT.TOROW()` to spill the response across cells to the right or `=PROMPT.TORANGE()` to let the model choose whether to spill multiple values (if any) across rows and/or columns or not.

<Warning>
  If you wish to combine multi-value output with function calling, you must use OpenAI as no other provider supports combining structured output with function calling.
</Warning>

### Function calling
Beyond basic text processing, you can use "Function Calling" to give models access to external tools and data and even take action in other systems. For example, expand the "Functions" menu in the Cellm ribbon tab, and click on "File Search" to enable the model to search for files on your computer. Then type:

````mdx With function calling
=PROMPT("Which pdf files do I have in C:\Users\username\Downloads?")
```` 

<Tip>
  Gemma 3 4B does not support function calling. For function calling you must use another model, e.g. OpenAI's `gpt-4o-mini`.
</Tip>

## Next steps

<CardGroup cols={2}>
  <Card
    title="Prompting"
    icon="pen-to-square"
    href="/usage/prompting">
    Learn more about prompting techniques and building automated workflows. 
  </Card>
  <Card
    title="Breaking Down Tasks for AI"
    icon="lightbulb"
    href="/usage/breaking-down-tasks">
    Learn how to split a complex task into multiple smaller ones and select the right model for your tasks' complexity.
  </Card>
  <Card
    title="Function Calling"
    icon="image"
    href="/usage/function-calling">
    Use Excel as a low-code task orchestrator by enabling Cellm to access external tools and data.
  </Card>
</CardGroup>
